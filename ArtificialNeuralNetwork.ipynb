{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9Um80Y8BCYnSc3fWx4E96"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PXRYHkHaO2ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Packages\n"
      ],
      "metadata": {
        "id": "16A6-SWoOyfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import feature # This pacakge is used for LBP feature extraction\n",
        "from sklearn import svm # This pacakge is used for svm classification\n",
        "from sklearn import metrics\n",
        "import sys\n",
        "%matplotlib inline\n",
        "import cv2\n",
        "import seaborn as sns # This pacakge is used for better visualization of data (e.g confusion matrix)\n",
        "import tensorflow as tf\n",
        "%load_ext tensorboard\n",
        "from numpy import asarray"
      ],
      "metadata": {
        "id": "cqbzO90oO3R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# Mouting the drive and unzipping the dataset"
      ],
      "metadata": {
        "id": "WwufqgoyR1us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We mount our google drive to have access to the data files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "edkWvYkcO5iI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the directory to the file directory.\n",
        "%cd filepath\n"
      ],
      "metadata": {
        "id": "1f_hE6XPP60O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip the dataset you have\n",
        "!pwd\n",
        "!unzip dataset.zip"
      ],
      "metadata": {
        "id": "QL-NnI39SPsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing and Splitting\n"
      ],
      "metadata": {
        "id": "ah1KyoFnP-_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We use os to get pathname as well as the folder name which we will be using as labels for our classifiers.\n",
        "#Change the folder_path to the actual pathname where we unzipped the dataset.\n",
        "#If unable to get folder names using this, please refer to Python notebook for SVM where I have defined an alternative way to get class names.\n",
        "symbols = []\n",
        "import os\n",
        "folder_path= #filepath                #Change filepath to the filepath you have.\n",
        "for file in os.listdir(folder_path):\n",
        "    folder = os.path.join(folder_path, file)\n",
        "    if os.path.isdir(folder):\n",
        "        symbols.append(os.path.basename(folder))\n",
        "print (symbols)\n"
      ],
      "metadata": {
        "id": "ycqWdUSKQDqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "#This section of the program will retrieve inidiviual images from all the folders into the variable 'data'.\n",
        "path_actual=#filepath                #Change filepath to the filepath you have.\n",
        "def getData():\n",
        "  import os\n",
        "  data = []\n",
        "  count = 0\n",
        "  for folder in os.listdir (path_actual):\n",
        "      for symbol in symbols:\n",
        "        if symbol in folder:\n",
        "          symbol_class = symbols.index(symbol)\n",
        "          path= os.path.join (path_actual,symbol)\n",
        "          for filename in os.listdir(path):\n",
        "            count+=1\n",
        "            image = cv2.imread(os.path.join(path,filename),cv2.IMREAD_GRAYSCALE)\n",
        "            data.append([image,symbol_class])\n",
        "  print(count)\n",
        "  return data"
      ],
      "metadata": {
        "id": "A2PjRaMfQGl3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This section of the code will be used for splitting the data set between training set, validation set and test set. Since the dataset has a total of 5000 images.\n",
        "#3000 images will be used for testing, 1000 for validation and 1000 for testing. The code ensures that the 3000 images used for training will be proportionate across all classes.\n",
        "#x_train and y_train are the training data and labels respectively, and x_test and y_test are test data and labels respectively.\n",
        "#x_valid and y_valid are validation data and labels.\n",
        "def split_dataset (dataset):\n",
        "  import random\n",
        "  random.seed(128)\n",
        "  random.shuffle(dataset)\n",
        "  x_train =[]\n",
        "  y_train =[]\n",
        "  x_valid=[]\n",
        "  y_valid=[]\n",
        "  x_test=[]\n",
        "  y_test=[]\n",
        "  test_dataset= []\n",
        "  for image,label in dataset:\n",
        "      if y_train.count(label)<300:\n",
        "        x_train.append(image)\n",
        "        y_train.append(label)\n",
        "      elif y_valid.count(label)<100:\n",
        "        x_valid.append(image)\n",
        "        y_valid.append(label)\n",
        "      else:\n",
        "        x_test.append(image)\n",
        "        y_test.append(label)\n",
        "\n",
        "        test_dataset.append([image,label])\n",
        "\n",
        "\n",
        "  return x_train,y_train,x_valid,y_valid,x_test,y_test"
      ],
      "metadata": {
        "id": "lVfJecqzQJQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are initializing 'Data2' to call the getData() function we created above.\n",
        "Data_ANN= getData()\n"
      ],
      "metadata": {
        "id": "gpMSjiibQPsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are using the split_dataset  function with the 'Data1' object we created above which will return our training and testing parameters.\n",
        "x_train, y_train,x_valid,y_valid, x_test, y_test = split_dataset(Data_ANN)\n",
        "print (len(x_train))\n",
        "\n"
      ],
      "metadata": {
        "id": "Sk4nnLuhQTp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ],
      "metadata": {
        "id": "La1fpKTbUjS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view few images and print its corresponding label\n",
        "img_index = 20\n",
        "fig = plt.figure()\n",
        "ax1 = fig.add_subplot(2,2,1)\n",
        "ax1.axis('off')\n",
        "ax1.imshow(x_train[img_index])\n",
        "print(symbols[y_train[img_index]])\n",
        "\n",
        "ax2 = fig.add_subplot(2,2,2)\n",
        "ax2.axis('off')\n",
        "img_index = 30\n",
        "ax2.imshow(x_train[img_index])\n",
        "print(symbols[y_train[img_index]])\n",
        "\n",
        "ax2 = fig.add_subplot(2,2,3)\n",
        "ax2.axis('off')\n",
        "img_index = 112\n",
        "ax2.imshow(x_train[img_index])\n",
        "print(symbols[y_train[img_index]])\n",
        "\n",
        "ax2 = fig.add_subplot(2,2,4)\n",
        "ax2.axis('off')\n",
        "img_index = 400\n",
        "ax2.imshow(x_train[img_index])\n",
        "print(symbols[y_train[img_index]])\n"
      ],
      "metadata": {
        "id": "vEx8xRceHbWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Raw pixels features\n"
      ],
      "metadata": {
        "id": "ujE863Vf2GFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying the shape of raw images\n",
        "print (np.shape(x_train))\n",
        "print (np.shape(x_valid))\n",
        "print (np.shape(x_test))\n",
        "print (np.shape(y_train))\n",
        "print (np.shape(y_valid))\n",
        "print (np.shape(y_test))\n",
        "\n",
        "#Creating another instance of images and labels to be used for raw image based ANN.\n",
        "x_train_raw = np.array(x_train)\n",
        "x_test_raw = np.array(x_test)\n",
        "y_train_raw = np.array(y_train)\n",
        "y_test_raw = np.array(y_test)\n",
        "x_valid_raw = np.array(x_valid)\n",
        "y_valid_raw = np.array(y_valid)"
      ],
      "metadata": {
        "id": "5iTfWE2z2Rug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating ANN for raw pixels features."
      ],
      "metadata": {
        "id": "TAcDaehGUuEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a model for the raw pixels features\n",
        "model_raw = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=[45,45]),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "metadata": {
        "id": "zsei9ct32xVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model\n",
        "model_raw.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "H_raw=model_raw.fit(x_train_raw, y_train_raw, epochs=30,validation_data=(x_valid_raw, y_valid_raw))\n",
        "#We can see that the accuracy remains the same after around 30 epochs and loss starts to increase.\n",
        "#Since we require higher epochs for the following models, the number of epochs is going to remain constant in all the models for ANN.\n",
        "#In real life, if there is no decrease in loss for 5 consecutive epochs, we would stop the training of the model.\n",
        "#We started with 45 epochs, but the model started overfitting after 30 epochs. So, we will be using 30 epochs."
      ],
      "metadata": {
        "id": "fPPmU7cv2956"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning curves for ANN classifier with raw features"
      ],
      "metadata": {
        "id": "yuYX1aBNU5en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the learning curves\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.DataFrame(H_raw.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()\n",
        "\n",
        "## Plot only the loss train loss\n",
        "plt.plot(H_raw.history['loss'])\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title(\"Cost/Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D6lETW3u4xxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_raw.summary()"
      ],
      "metadata": {
        "id": "i3WhT9bs5Fha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of ANN with raw features"
      ],
      "metadata": {
        "id": "HhpvVt8uVHQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_raw.evaluate(x_test_raw, y_test_raw)\n",
        "#The model returns an accuracy of 0.71 which is close to the training model accuracy of 0.80. We can notice the slight signs of overfitting."
      ],
      "metadata": {
        "id": "-IPbG_2N5LbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preproccesing for Normalization"
      ],
      "metadata": {
        "id": "Pf07dLI5VLX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COnverting all training and testing data into array from list.\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "x_valid = np.array(x_valid)\n",
        "y_valid = np.array(y_valid)\n",
        "x_test = np.array (x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "#Reshaping the image into a 1D array for normalization.\n",
        "x_train = x_train.reshape(len(x_train),-1)\n",
        "x_valid=x_valid.reshape(len(x_valid),-1)\n",
        "x_test= x_test.reshape(len(x_test),-1)\n",
        "\n",
        "print(np.shape(x_train))\n",
        "print (np.shape(y_train))"
      ],
      "metadata": {
        "id": "T3ihY5R7HaQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization of data\n"
      ],
      "metadata": {
        "id": "HOPsH5IPQUi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This section of the code is to perform normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform (x_train)\n",
        "x_valid = scaler.transform(x_valid)\n",
        "x_test = scaler.transform (x_test)"
      ],
      "metadata": {
        "id": "V0kIWJQ5bIVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This section of the code is to transform the array back to 2d array to be used for LBP since we already know that the shape of image is 45* 45\n",
        "image_shape = [45,45]\n",
        "x_train = x_train.reshape (-1, image_shape[0], image_shape[1])\n",
        "x_valid = x_valid.reshape (-1, image_shape[0], image_shape[1])\n",
        "x_test = x_test.reshape (-1, image_shape[0], image_shape[1])"
      ],
      "metadata": {
        "id": "dKYl2c95Kmjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LBP Feature extraction for Neural Networks"
      ],
      "metadata": {
        "id": "nCTHQs4ybLLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LBP CLASS DEFINITION\n",
        "class LocalBinaryPatterns:\n",
        "\tdef __init__(self, points, radius):\n",
        "\t\t# store the number of points and radius\n",
        "\t\tself.points = points\n",
        "\t\tself.radius = radius\n",
        "\n",
        "\tdef LBPfeatures(self, image, eps=1e-7):\n",
        "\t\t# compute the Local Binary Pattern representation\n",
        "\t\t# of the image, and then use the LBP representation\n",
        "\t\t# to build the histogram of patterns\n",
        "\t\tlbp = feature.local_binary_pattern(image, self.points,\n",
        "\t\t\tself.radius, method=\"uniform\")\n",
        "    # Form the histogram\n",
        "\t\t(hist, _) = np.histogram(lbp.ravel(),\n",
        "\t\t\tbins=np.arange(0, self.points + 3),\n",
        "\t\t\trange=(0, self.points + 2))\n",
        "\n",
        "\t\t# normalize the histogram\n",
        "\t\thst = hist.astype(\"float\")\n",
        "\t\thst /= (hist.sum() + eps)\n",
        "\n",
        "\t\t# return the histogram of Local Binary Patterns\n",
        "\t\treturn hst"
      ],
      "metadata": {
        "id": "zr3fJiJ-bQ1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object of LocalBinaryPatterns class for training data set and initial the parameters.\n",
        "desc = LocalBinaryPatterns(24, 8)\n",
        "data_train = []\n",
        "labels_train = []\n",
        "\n",
        "\n",
        "# loop over the training images\n",
        "for img_index in range(len(x_train)):\n",
        "\t# load the train image, and extract LBP features\n",
        "    image = (x_train [img_index])\n",
        "    hist = desc.LBPfeatures (image)\n",
        "\n",
        "\n",
        "\t# extract the label from the image path, then update the\n",
        "\t# label and data lists\n",
        "    labels_train.append(y_train[img_index])\n",
        "    data_train.append(hist)\n",
        "\n",
        "print (np.shape(data_train))"
      ],
      "metadata": {
        "id": "Y2kVTQCfbYhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object of LocalBinaryPatterns class for validation data set and initial the parameters.\n",
        "desc = LocalBinaryPatterns(24, 8)\n",
        "data_valid = []\n",
        "labels_valid = []\n",
        "\n",
        "\n",
        "# loop over the training images\n",
        "for img_index in range(len(x_valid)):\n",
        "\t# load the train image, and extract LBP features\n",
        "    image = (x_valid [img_index])\n",
        "    hist = desc.LBPfeatures (image)\n",
        "\n",
        "\n",
        "\t# extract the label from the image path, then update the\n",
        "\t# label and data lists\n",
        "    labels_valid.append(y_valid[img_index])\n",
        "    data_valid.append(hist)\n",
        "\n",
        "print (np.shape(data_valid))"
      ],
      "metadata": {
        "id": "k3DjKRvRrE6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an object of LocalBinaryPatterns class for test data set and initial the parameters.\n",
        "desc = LocalBinaryPatterns(24, 8)\n",
        "data_test = []\n",
        "labels_test = []\n",
        "\n",
        "\n",
        "# loop over the training images\n",
        "for img_index in range(len(x_test)):\n",
        "\t# load the train image, and extract LBP features\n",
        "    image = (x_test [img_index])\n",
        "    hist = desc.LBPfeatures (image)\n",
        "\n",
        "\n",
        "\t# extract the label from the image path, then update the\n",
        "\t# label and data lists\n",
        "    labels_test.append(y_test[img_index])\n",
        "    data_test.append(hist)\n",
        "\n",
        "print (np.shape(data_test))\n",
        "print (np.shape(labels_test))"
      ],
      "metadata": {
        "id": "uY0QL8kTkZm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reshaping the data for the model\n"
      ],
      "metadata": {
        "id": "2joqyq9DSOtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Reshaping the data retrieved after LBP for the model\n",
        "data_train = np.array(data_train)\n",
        "data_valid= np.array(data_valid)\n",
        "labels_train = np.array(labels_train)\n",
        "labels_valid = np.array (labels_valid)\n",
        "data_train = np.reshape(data_train, (-1,26,1))\n",
        "data_valid = np.reshape(data_valid, (-1,26,1))\n",
        "print (np.shape(data_train))\n",
        "print (np.shape(data_valid))\n",
        "print (type (data_train))\n",
        "print (type (data_valid))\n",
        "print (type (labels_train))\n",
        "print (type (labels_valid))"
      ],
      "metadata": {
        "id": "aFqyK0piSShg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# Creating ANN for LBP feature extraction\n"
      ],
      "metadata": {
        "id": "F5BWlbYLs9J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_LBP = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=[26,1]),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "metadata": {
        "id": "JoMDyxe6tB4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_LBP.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "H=model_LBP.fit(data_train, labels_train, epochs=30,validation_data=(data_valid, labels_valid))\n",
        "#Since we are using 30 epochs for raw features, and we are trying to compare the output between different features, it is important to use the same parameters.\n",
        "#Hence, we are using 30 epochs in this model.\n",
        "#In real life, since the loss curve is still going down, we should train for more epochs before loss stops decreasing."
      ],
      "metadata": {
        "id": "A4f7whuRtf2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning curves for ANN classifier with LBP feature extraction"
      ],
      "metadata": {
        "id": "PpRkpUGxVkp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the learning curves\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.DataFrame(H.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()\n",
        "\n",
        "## Plot only the loss train loss\n",
        "plt.plot(H.history['loss'])\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title(\"Cost/Loss Curve\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l1U4TBZcErcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Use model.summary to great a summary for the model(layers, type, shape, etc.)\n",
        "model_LBP.summary()"
      ],
      "metadata": {
        "id": "a8W8sVUYE9CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#Evaluation of ANN with LBP feature extraction"
      ],
      "metadata": {
        "id": "Ndu81VUfFCFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = np.array (data_test)\n",
        "labels_test = np.array (labels_test)\n",
        "data_test = np.reshape(data_test,(-1,26,1))\n",
        "print (np.shape(data_test))"
      ],
      "metadata": {
        "id": "g1D1vl_LFHDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_LBP.evaluate(data_test, labels_test)"
      ],
      "metadata": {
        "id": "dUJtrpz8FxtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HOG feature extraction for ANN"
      ],
      "metadata": {
        "id": "n_sBNzxIiAV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train, x_valid and x_train variables have the training, validation and testing images respectively.\n",
        "# initialize the data matrix and labels\n",
        "print(\"Extracting features from training dataset...\")\n",
        "hog_data_train = []\n",
        "hog_labels_train = []\n",
        "\n",
        "# loop over the training images\n",
        "for img_index in range(len(x_train)):\n",
        "  # load the image, and extract HOG features it\n",
        "\n",
        "  image = (x_train [img_index]) # Complete the code\n",
        "\n",
        "  # Hint: use orientation =9, pixel_per_cell=(10,10), cells_per_block=2,2,\n",
        "  # transform_sqrt=True and block_norm=\"L2-Hys\"\n",
        "  #\n",
        "  H = feature.hog(image, orientations=9, pixels_per_cell=(10,10), cells_per_block=(2,2), transform_sqrt=False, block_norm=\"L2-Hys\")\n",
        "\n",
        "  # update the data and labels\n",
        "  hog_data_train.append(H) # Complete the code\n",
        "  hog_labels_train.append(y_train[img_index]) # Complete the code\n",
        "## END YOUR CODE HERE ##\n",
        "print(np.shape(hog_data_train))\n",
        "print(np.shape(hog_labels_train))"
      ],
      "metadata": {
        "id": "S2PWWqHEiD_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train, x_valid and x_train variables have the training, validation and testing images respectively.\n",
        "# initialize the data matrix and labels\n",
        "print(\"Extracting features from validation dataset...\")\n",
        "hog_data_valid = []\n",
        "hog_labels_valid = []\n",
        "\n",
        "# loop over the training images\n",
        "for img_index in range(len(x_valid)):\n",
        "  # load the image, and extract HOG features it\n",
        "\n",
        "  image = (x_valid [img_index]) # Complete the code\n",
        "\n",
        "  # Hint: use orientation =9, pixel_per_cell=(10,10), cells_per_block=2,2,\n",
        "  # transform_sqrt=True and block_norm=\"L2-Hys\"\n",
        "  #\n",
        "  H = feature.hog(image, orientations=9, pixels_per_cell=(10,10), cells_per_block=(2,2), transform_sqrt=False, block_norm=\"L2-Hys\")\n",
        "\n",
        "  # update the data and labels\n",
        "  hog_data_valid.append(H) # Complete the code\n",
        "  hog_labels_valid.append(y_valid[img_index]) # Complete the code\n",
        "## END YOUR CODE HERE ##\n",
        "print(np.shape(hog_data_valid))\n",
        "print(np.shape(hog_labels_valid))"
      ],
      "metadata": {
        "id": "r7HJZHEsjCMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train, x_valid and x_train variables have the training, validation and testing images respectively.\n",
        "# initialize the data matrix and labels\n",
        "print(\"Extracting features from testing dataset...\")\n",
        "hog_data_test = []\n",
        "hog_labels_test = []\n",
        "\n",
        "# loop over the training images\n",
        "for img_index in range(len(x_test)):\n",
        "  # load the image, and extract HOG features it\n",
        "\n",
        "  image = (x_test [img_index]) # Complete the code\n",
        "\n",
        "  # Hint: use orientation =9, pixel_per_cell=(10,10), cells_per_block=2,2,\n",
        "  # transform_sqrt=True and block_norm=\"L2-Hys\"\n",
        "  #\n",
        "  H = feature.hog(image, orientations=9, pixels_per_cell=(10,10), cells_per_block=(2,2), transform_sqrt=False, block_norm=\"L2-Hys\")\n",
        "\n",
        "  # update the data and labels\n",
        "  hog_data_test.append(H) # Complete the code\n",
        "  hog_labels_test.append(y_test[img_index]) # Complete the code\n",
        "## END YOUR CODE HERE ##\n",
        "print(np.shape(hog_data_test))\n",
        "print(np.shape(hog_labels_test))"
      ],
      "metadata": {
        "id": "6abpIWGLjQpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data transformation for HOG feature extraction"
      ],
      "metadata": {
        "id": "1ePb7Xj_V9IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaping the training, validation and testing images before feeding them into the model\n",
        "hog_data_train = np.array(hog_data_train)\n",
        "hog_data_valid= np.array(hog_data_valid)\n",
        "hog_data_test =np.array(hog_data_test)\n",
        "hog_labels_train = np.array(hog_labels_train)\n",
        "hog_labels_valid = np.array (hog_labels_valid)\n",
        "hog_labels_test = np.array (hog_labels_test)\n",
        "hog_data_train = np.reshape(hog_data_train, (-1,324,1))\n",
        "hog_data_valid = np.reshape(hog_data_valid, (-1,324,1))\n",
        "hog_data_test = np.reshape(hog_data_test, (-1,324,1))\n",
        "print (np.shape(hog_data_train))\n",
        "print (np.shape(hog_data_valid))\n",
        "print (type (hog_data_train))\n",
        "print (type (hog_data_valid))\n",
        "print (type (hog_labels_train))\n",
        "print (type (hog_labels_valid))"
      ],
      "metadata": {
        "id": "YbhtuMt6jnZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating ANN for HOG feature extration"
      ],
      "metadata": {
        "id": "bV1_VGgcWDJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_hog = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=[324,1]),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
      ],
      "metadata": {
        "id": "cDuYT3jBkigk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_hog.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "H_hog=model_hog.fit(hog_data_train, hog_labels_train, epochs=30,validation_data=(hog_data_valid, hog_labels_valid))"
      ],
      "metadata": {
        "id": "tVUcdHfdkull"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning curves for ANN classifier with HOG feature extraction"
      ],
      "metadata": {
        "id": "Y79NCOGlWJPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the learning curves\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "pd.DataFrame(H_hog.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
        "plt.show()\n",
        "\n",
        "## Plot only the loss train loss\n",
        "plt.plot(H_hog.history['loss'])\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title(\"Cost/Loss Curve\")\n",
        "plt.show()\n",
        "#From the learning curve, we can observe that the accuracy stops increasing and the loss stops decreasing after a few epochs.\n",
        "#In real world, we would reduce the number of epochs to reduce the computation load for this model. The ideal epochs would be around 10 since the loss started increasing after it.\n",
        "#But, to keep the parameters similar between models for different features, we are going to leave the epochs as it is."
      ],
      "metadata": {
        "id": "Hw7tILM-mipF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating on a test dataset\n",
        "\n",
        "model_hog.evaluate(hog_data_test, hog_labels_test)\n",
        "#We get an accuracy of 0.9900 for a test set which is similar to the accuracy we received for training set  which is 0.9970\n",
        "#This emphasizes the importance of HOG feature extraction since it extracts more features than LBP and has a higher accuracy.\n"
      ],
      "metadata": {
        "id": "dIJzDAJupIUu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}